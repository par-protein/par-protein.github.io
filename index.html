<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="description" content="Protein Autoregressive Modeling via Multiscale Structure Generation">
    <meta property="og:title" content="Protein Autoregressive Modeling via Multiscale Structure Generation">
    <meta property="og:description" content="Protein Autoregressive Modeling via Multiscale Structure Generation">
    <!-- <meta property="og:image" content="https://tao-amodal.github.io/static/images/webpage_preview.png"> -->
    <meta property="twitter:title" content="Protein Autoregressive Modeling via Multiscale Structure Generation">
    <meta property="twitter:description" content="Protein Autoregressive Modeling via Multiscale Structure Generation">
    <!-- <meta property="twitter:image" content="https://tao-amodal.github.io/static/images/webpage_preview.png"> -->
    <meta property="og:type" content="website">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="keywords"
        content="PAR, ICML, Protein Autoregressive Modeling, Next-Scale Prediction, Protein Backbone Generation, Flow Matching, Protein">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>Protein Autoregressive Modeling via Multiscale Structure Generation</title>
    <!-- <link rel="icon" type="image/x-icon" href="./static/images/car_icon.png"> -->

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-D65ZW4CJYF"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-D65ZW4CJYF');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="./static/css/animation_style.css"> <!-- Animation styles -->
    <link rel="stylesheet" href="./static/css/style.css"> <!-- Animation styles -->

    <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> -->
    <script src="./static/js/jquery-3.6.4.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/lazy.js"></script>
    <script src="./static/js/faster.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/animation_script.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

</head>

<body>

    <section class="hero" style="border-bottom: 1px solid #dbdbdb;">
        <div class="hero-body" style="padding-bottom: 1.2rem;">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            Protein Autoregressive Modeling <br>via Multiscale Structure Generation
                        </h1>
                        <div class="is-size-6 publication-authors">
                            (to update) <span class="author-block">
                                Yanru Qu, </span>
                            <span class="author-block">
                                Cheng-Yen Hsieh, </span>
                            <span class="author-block">
                                Zaixiang Zheng, </span>
                            <span class="author-block">
                                Ge Liu, </span>
                            <span class="author-block">
                                Quanquan Gu, </span>
                            <br>
                        </div>
                        <!-- <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://wesleyhsieh0806.github.io/">Wesley Hsieh</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=5JsycngAAAAJ">Xinyou Wang</a><sup>1,2</sup>, </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=hluDH7EAAAAJ">Daiheng Zhang</a><sup>1,3</sup>, </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Dongyu_Xue1">Dongyu Xue</a><sup>1</sup>, </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=4MB4orsAAAAJ">Fei Ye</a><sup>1</sup>, </span>
              <span class="author-block">
                <a href="http://nlp.nju.edu.cn/huangsj/">Shujian Huang</a><sup>2</sup>, </span>
              <span class="author-block">
                <a href="https://zhengzx-nlp.github.io/">Zaixiang Zheng</a><sup>1</sup>, </span>
              <span class="author-block">
                <a href="https://web.cs.ucla.edu/~qgu/">Quanquan Gu</a><sup>1</sup>, </span><br>
            </div> -->

                        <!-- <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Carnegie Mellon University &nbsp; </span>
              <span class="author-block"><sup>2</sup>Toyota Research Institute &nbsp; </span>
              <span class="author-block"><small style="font-size: 0.8em;"><br><sup>*</sup>Equal Contribution</small></span>
              <span class="author-block"><small style="font-size: 0.8em;"><br><sup>‚Ä†</sup>Project Lead</small></span>
              <span class="author-block"><small style="font-size: 0.8em;"><br><sup>‚Ä°</sup>Core Contributor</small></span>
            </div> -->
                        <div class="is-size-6 publication-authors">
                            <span class="author-block"><sup>1</sup>ByteDance Research </span>
                            &nbsp;&nbsp;
                            <span class="author-block"><sup>2</sup>University of Illinois at Urbana-Champaign </span>
                            &nbsp;&nbsp;
                            <!-- <span class="eql-cntrb"><small style="font-size: 0.75em;"><br>
                  <sup>*</sup>Equal Contribution
                  &nbsp;
                  <sup>‚Ä†</sup>Project Lead
                  &nbsp;
                  <sup>‚Ä°</sup>Core Contributor
                  &nbsp;
                  <sup>#</sup>Corresponding Author
                </small>
              </span> -->
                        </div>
                        <!-- <h1 style="font-size:24px">ICCV 2023 (<b>Oral, Best Student Paper</b>)</h1> -->
                        <h2 class="subtitle is-3 has-text-weight-semibold mt-1 mb-1" style="color: #F05365;"> ICLR 2026
                            (<b>Submission</b>) </h2>
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper (Incoming)</span>
                                    </a>
                                </span>
                                <!-- Video Link. -->
                                <!-- <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=KHoAG3gA024"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code (Incoming)</span>
                                    </a>
                                </span>
                                <!-- Model Link. -->
                                <span class="link-block">
                                    <a href="#" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">ü§ó</span>
                                        <span>Model Checkpoints (Incoming)</span>
                                    </a>
                                </span>
                                <!-- Citation Link. -->
                                <span class="link-block">
                                    <a href="#Citation" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            üìö
                                        </span>
                                        <span>Citation</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                        <!-- <span class="publication-venue" style="font-weight: 500; font-size: 1.9ch; ; margin-top: 0.5rem; margin-bottom: 0.3rem; display: inline-block;">
              <strong>Design choices are essential:</strong> Our designs enable the 650M multimodal PLM to <br>outperform 3B-scale baselines and specialized structure folding models.
            </span> -->
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- <style>
    .video-grid {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      /* Three columns */
      grid-template-rows: repeat(1, 1fr);
      /* Two rows */
      gap: 0px 4px;
      /* Gap between videos */
      width: 80%;
      /* Set the container width to 80% */
      margin: 0 auto;
      /* Center the container horizontally */
    }

    .video-grid video {
      width: 100%;
      /* Videos fill the container width */
      height: auto;
    }
  </style> -->

    <br>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            We present protein autoregressive modeling (PAR), the first multi-scale autore-
                            gressive framework for protein backbone generation via coarse-to-fine next-scale
                            prediction. Using the hierarchical nature of proteins, PAR generates structures that
                            mimic sculpting a statue, forming a coarse topology and refining structural details
                            over scales. To achieve this, PAR consists of three key components: (i) multi-
                            scale downsampling operations that represent protein structures across multiple
                            scales during training; (ii) an autoregressive transformer that encodes multi-scale
                            information and produces conditional embeddings to guide structure generation;
                            (iii) a flow-based backbone decoder that generates backbone atoms conditioned
                            on these embeddings. Moreover, autoregressive models suffer from exposure bias,
                            caused by the training and the generation procedure mismatch, and substantially de-
                            grades structure generation quality. We effectively alleviate this issue by adopting
                            noisy context learning and scheduled sampling, enabling robust backbone gener-
                            ation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible
                            human-prompted conditional generation and motif scaffolding without requiring
                            fine-tuning. On the unconditional generation benchmark, PAR effectively learns
                            protein distributions and produces backbones of high design quality, and exhibits
                            favorable scaling behavior. Together, these properties establish PAR as a promising
                            framework for protein structure generation. </p>
                    </div>
                </div>
            </div>
            <br>
            <!-- Paper video. -->
            <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <a id="overview_video"></a>
            <iframe src="https://www.youtube.com/embed/KHoAG3gA024">
            </iframe>
          </div>
        </div> -->
        </div>
        </div>
        <!--/ Paper video. -->
    </section>

    <style>
        img {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    </style>

    <section>
        <div class="container is-max-desktop content">
            <div class="column is-full-width">
                <div class="toc content mb-5"> <!-- mb-5 adds margin below the TOC -->
                    <h2 class="title is-3" id="Overview">Table of Contents</h2>
                    <ul>
                        <li><a href="#motivation">Motivation</a></li>
                        <li><a href="#contributions">Major Contributions</a></li>
                        <li><a href="#par">Protein Autoregressive Modeling</a></li>
                        <ul>
                            <li><a href="#decomposer">Protein Decomposer</a></li>
                            <li><a href="#encoder">Autoregressive Transformer Encoder</a></li>
                            <li><a href="#decoder">Flow Decoder</a></li>
                            <li><a href="#bias">Bias Mitigation</a></li>
                            <li><a href="#inference">Inference</a></li>
                        </ul>
                        <li><a href="#experiments">Experiments</a></li>
                        <ul>
                            <li><a href="#uncond">Protein Backbone Generation</a></li>
                            <li><a href="#prompt">0-shot Prompted Generation</a></li>
                            <li><a href="#motif">0-shot Motif Scaffolding</a></li>
                            <li><a href="#scaling">Scaling Effects of PAR</a></li>
                            <li><a href="#efficiency">Inference Efficiency</a></li>
                            <li><a href="#attn">Interpreting AR Transformer Attention</a></li>
                        </ul>
                        <li><a href="#conclusion">Conclusion</a></li>
                        <li><a href="#citation">BibTeX</a></li>
                    </ul>
                    <hr> <!-- Optional: Add a horizontal rule after the TOC -->
                </div>
                <h2 class="title is-3" id="motivation"> Motivation </h2>
                <div class="content has-text-justified">
                    <p>
                        Deep generative modeling of proteins has emerged as a way to design and model novel structures
                        with desired functions and properties.
                        A widely adopted approach is to directly model the distribution of three-dimensional protein
                        structures, mostly based on diffusion models and their variations (e.g., flow matching).
                        On the other hand, though autoregressive (AR) modeling showing striking scalability, and
                        zero-shot generalization in large language models,
                        AR modeling has received little attention in backbone modeling for 2 main reasons:
                        (i) Extending AR models to continuous data often relies on <b>data discretization</b>, which can
                        reduce structural fidelity and fine-grained details for proteins.
                        (ii) Protein residues exhibit strong <b>bidirectional dependencies</b>, while AR model usually
                        follows a unidirectional order.
                        Inspired by the Visual AutoRegressive Model (VAR) (todo:cite?), we raise the following research
                        question:
                        Could we start from a coarse-grained representation that captures global topology, and
                        iteratively refine it through <b>next-scale prediction</b> to obtain accurate backbone
                        geometries?
                    </p>
                    <figure style="margin-top: 1rem; margin-bottom: 1rem;">
                        <img src="./static/img/generation_over_scale_with_rmsd_4_samples.png"
                            alt="Multi-scale Generation" style="width:100%;">

                        <!-- Add the figcaption below the image -->
                        <figcaption class="has-text-centered is-size-7 mt-2">
                            <strong>Figure 1: Multi-scale Generation with PAR.</strong>
                        </figcaption>
                    </figure>
                </div>
                <h2 class="title is-3" id="contributions"> Major Contributions </h2>
                <div class="content has-text-justified">
                    <p>
                        Our main contributions are summarized as follows:
                    </p>
                    <ul>
                        <li>We present PAR, the first multi-scale AR model for protein backbone generation that
                            addresses key limitations of existing AR methods.</li>
                        <li>PAR comprises multi-scale downsampling, AR transformer, and a flow-based decoder, to
                            directly model CŒ± atoms, avoiding fidelity loss from discretization.</li>
                        <li>We alleviate exposure bias through noisy context learning and scheduled sampling,
                            effectively improving structure generation.</li>
                        <li>Our model shows an interpretable generation process that forms coarse backbone topology and
                            refines it progressively.</li>
                        <li>Benchmarking results show that PAR effectively captures protein data distributions,
                            achieving FPSD score of 231.5 against PDB dataset that further scale with training compute.
                        </li>
                        <li>PAR exhibits zero-shot generalization potential, reflecting the versatility of AR large
                            language models.</li>
                    </ul>
                </div>

                <div>
                    <h2 class="title is-3" id="par"> Protein Autoregressive Modeling</h2>
                    <figure style="margin-top: 1rem; margin-bottom: 1rem;">
                        <img src="./static/img/par_with_example.png" style="width:100%;">

                        <!-- Add the figcaption below the image -->
                        <figcaption class="has-text-centered is-size-7 mt-2">
                            <strong>Figure 2: Model Architecture of PAR.</strong>
                            <p> PAR comprises the autoregressive (AR)
                                transformer <em>T<sub>Œ∏</sub></em> (left) and the flow-based backbone decoder
                                <i>v<sub>Œ∏</sub></i> (right). During training, we downsample a backbone
                                <b>x</b> ‚àà ‚Ñù<sup>L√ó3</sup> into multi-scale representations {x<sup>1</sup>, ‚Ä¶,
                                x<sup>n</sup>}.
                                <i>AR transformer</i> performs next-scale prediction, producing conditional embeddings
                                (z<sup>1</sup>, ‚Ä¶, z<sup>n</sup>) from (bos, ‚Ä¶, x<sup>n‚àí1</sup>). The shared
                                <i>flow-based decoder</i> learns to denoise backbones x<sup>i</sup> at each scale
                                conditioned
                                on z<sup>i</sup>. At inference, PAR autoregressively generates x<sup>i</sup> until the
                                final
                                structure x is constructed.
                            </p>

                        </figcaption>
                    </figure>

                    <h3 class="title is-4" id="decomposer"> Protein Decomposer</h3>
                    <div class="content has-text-justified">
                        <div
                            style="border-left: 4px solid #ccc; padding: 1em; background-color: #f9f9f9; font-weight: bold;">
                            $$
                            q_{\text{decompose}} : \mathbf{x} \rightarrow X = \{ x^{1}, x^{2}, \ldots, x^{n} \}
                            = \{ \text{Down}(\mathbf{x}, size(1)), \text{Down}(\mathbf{x}, size(2)), \ldots, \mathbf{x}
                            \}
                            $$

                        </div>

                        <p>
                            The input structure <b>x</b> ‚àà ‚Ñù<sup>L√ó3</sup> is decomposed into multi-scale
                            representations
                            {x<sup>1</sup>, ‚Ä¶, x<sup>n</sup>} through a scale configuration {size(1), size(2), ‚Ä¶,
                            size(n)},
                            where <i>Down(x, size(i))</i> ‚àà ‚Ñù<sup>size(i)√ó3</sup> denotes a downsampling operation that
                            interpolates <b>x</b> along the sequence dimension, leading to size(i) 3D centroids that
                            provide a coarse structural layout.
                        </p>
                        <p>
                            <strong>Scale configuration</strong> <b>S = {size(1), ‚Ä¶, size(n)}</b> could be defined in
                            two
                            ways.
                            When defined <i>by length</i>, scales are chosen as hyperparameters, e.g.,
                            <b>S = {64, 128, 256}</b>. In this case, if <b>L</b> lies in <b>(size(i), size(i + 1)]</b>,
                            the protein could be generated with only <b>i + 1</b> autoregressive steps.
                            When defined <i>by ratio</i>, scales are adaptively determined based on protein length,
                            e.g., <b>S = {L/4, L/2, L}</b>.
                            Empirically, defining scales by length yields slightly better results in modeling data
                            distributions. We adopt this as the default configuration.
                            This design enables training PAR with flexible scale configurations.
                            In the following sections, we describe how this hierarchy of representations are modeled
                            using the autoregressive transformer and backbone decoder.
                        </p>

                        <div class="container is-max-desktop content">
                            <figure class="table-container mt-5 mb-5">
                                <figcaption class="has-text-centered is-size-6 mb-3">
                                    <b>Table 1:</b>
                                    <strong>Multi-scale formulation.</strong>
                                    We ablate different strategies for scale configuration in downsampling.
                                </figcaption>

                                <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
                                    <thead>
                                        <tr>
                                            <th rowspan="2" class="is-vcentered">Define scale</th>
                                            <th colspan="1" class="has-text-centered">Designability<br>(% ‚Üë)</th>
                                            <th colspan="1" class="has-text-centered">sc-RMSD ‚Üì</th>
                                            <th colspan="2" class="has-text-centered">FPSD vs.</th>
                                            <th rowspan="2" class="has-text-centered">fS<br>(C / A / T ‚Üë)</th>
                                        </tr>
                                        <tr>
                                            <th class="has-text-centered">‚Äì</th>
                                            <th class="has-text-centered">‚Äì</th>
                                            <th class="has-text-centered">PDB ‚Üì</th>
                                            <th class="has-text-centered">AFDB ‚Üì</th>
                                        </tr>
                                    </thead>

                                    <tbody>
                                        <tr>
                                            <td>{64, 256}</td>
                                            <td class="has-text-centered">83.0</td>
                                            <td class="has-text-centered">1.38</td>
                                            <td class="has-text-centered">282.85</td>
                                            <td class="has-text-centered">274.32</td>
                                            <td class="has-text-centered">2.14 / 6.58 / 20.66</td>
                                        </tr>

                                        <tr>
                                            <td>{64, 128, 256}</td>
                                            <td class="has-text-centered">85.0</td>
                                            <td class="has-text-centered">1.39</td>
                                            <td class="has-text-centered">279.63</td>
                                            <td class="has-text-centered">267.35</td>
                                            <td class="has-text-centered">2.15 / 6.52 / 20.35</td>
                                        </tr>

                                        <tr>
                                            <td>{64, 128, 192, 256}</td>
                                            <td class="has-text-centered">77.8</td>
                                            <td class="has-text-centered">1.55</td>
                                            <td class="has-text-centered">296.70</td>
                                            <td class="has-text-centered">282.69</td>
                                            <td class="has-text-centered">2.05 / 6.04 / 18.69</td>
                                        </tr>

                                        <tr>
                                            <td>{64, 96, 128, 192, 256}</td>
                                            <td class="has-text-centered">81.0</td>
                                            <td class="has-text-centered">1.51</td>
                                            <td class="has-text-centered">276.00</td>
                                            <td class="has-text-centered">263.58</td>
                                            <td class="has-text-centered">2.17 / 6.31 / 20.65</td>
                                        </tr>

                                        <tr>
                                            <td>{L/4, L/2, L}</td>
                                            <td class="has-text-centered">86.4</td>
                                            <td class="has-text-centered">1.49</td>
                                            <td class="has-text-centered">310.64</td>
                                            <td class="has-text-centered">298.30</td>
                                            <td class="has-text-centered">2.00 / 5.87 / 18.91</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </figure>
                        </div>
                    </div>

                    <h3 class="title is-4" id="encoder"> AR Transformer Encoder</h3>
                    <div class="content has-text-justified">

                        <p>
                            To formulate the autoregressive order, we leverage the hierarchical nature of proteins,
                            where a protein structure could span various levels of representations from coarse
                            tertiary topology to the finest atomic coordinates. We adopt the next-scale prediction
                            to model per-scale distribution based on prior coarser scales, which further ensures
                            that the <i>bidirectional</i> dependencies of residues are modeled over each scale.
                            We train our autoregressive model (Fig.&nbsp;2, left), a non-equivariant transformer
                            <i>T<sub>Œ∏</sub></i>, to produce scale-wise conditioning embedding
                            <i>z<sup>i</sup></i> for scale <i>i</i> depending on prior scales
                            <b>X&lt;i = x<sup>1</sup>, ‚Ä¶, x<sup>i‚àí1</sup></b>:
                        </p>

                        <div
                            style="border-left: 4px solid #ccc; padding: 1em; background-color: #f9f9f9; font-weight: bold;">
                            $$
                            \mathbf{z}^i
                            = \mathcal{T}_{\theta}(X^{\lt i})
                            = \mathcal{T}_{\theta}\left(
                            [\mathrm{bos},\ \mathrm{Up}(x^{1}, \mathrm{size}(2)),\ \ldots,\ \mathrm{Up}(x^{i-1},
                            \mathrm{size}(i))]
                            \right)
                            $$
                        </div>

                        <p>
                            where <b>bos</b> ‚àà ‚Ñù<sup>size(1)√ó3</sup> is a learnable embedding,
                            and Up(x<sup>i‚àí1</sup>, size(i)) interpolates x<sup>i‚àí1</sup> to size(i) 3D points.
                            All inputs are concatenated along the sequence dimension before being fed into
                            T<sub>Œ∏</sub>. The embedding z<sup>i</sup> is then used to condition the flow
                            matching decoder to predict the backbone coordinates x<sup>i</sup>.
                        </p>

                        <!-- <div>
          <img src="./static/img/1a26.jpeg" alt="Teaser" style="width:80%;">
        </div> -->

                    </div>

                    <h3 class="title is-4" id="decoder"> Flow-based Atomic Decoder</h3>
                    <div class="content has-text-justified">
                        <p>
                            We enable PAR to directly model CŒ± positions <b>x</b>, wherein
                            p<sub>Œ∏</sub>(x | z<sup>i</sup>) is parameterized by an atomic decoder
                            v<sub>Œ∏</sub> with flow matching, which maps standard normal distribution
                            to the target data distribution. We condition v<sub>Œ∏</sub> with scale-wise
                            conditioning z<sup>i</sup> predicted by the AR Transformer T<sub>Œ∏</sub> at
                            each scale i (Fig. 2, right). During training, we sample the noise
                            Œµ<sup>i</sup> ~ ùí©(0, I) and a time variable t<sup>i</sup> ‚àà [0, 1], and compute
                            the interpolated sample as
                            x<sub>t<sup>i</sup></sub><sup>i</sup> = t<sup>i</sup> ¬∑ x<sup>i</sup> + (1 ‚àí t<sup>i</sup>)
                            ¬∑ Œµ<sup>i</sup>.
                            As such, we can jointly train v<sub>Œ∏</sub> and T<sub>Œ∏</sub> with an FM objective:
                        </p>

                        <div
                            style="border-left: 4px solid #ccc; padding: 1em; background-color: #f9f9f9; font-weight: bold;">
                            $$
                            \mathcal{L}(\theta)
                            = \mathbb{E}_{x \sim p_D} \left[
                            \frac{1}{n} \sum_{i=1}^{n}
                            \frac{1}{\text{size}(i)}
                            \mathbb{E}_{t^i \sim p(t^i),\, \epsilon^i \sim \mathcal{N}(0, I)}
                            \left\|
                            v_{\theta}\left(x_{t^i}^i,\, t^i,\, z^i \right)
                            - \left(x^i - \epsilon^i\right)
                            \right\|^2
                            \right]
                            $$
                        </div>

                        <p>
                            where p<sub>D</sub>(<b>x</b>) denotes the training data distribution and p(t)
                            denotes the t-sampling distribution in training.
                            The conditioning embedding z<sup>i</sup> is injected into the atomic decoder network
                            v<sub>Œ∏</sub> through adaptive layer norms.
                            <!-- We further concatenate a learnable scale embedding before feeding into the decoder. -->
                        </p>


                    </div>

                    <h3 class="title is-4" id="bias"> Bias Mitigation</h3>
                    <div class="content has-text-justified">
                        <p>
                            Training AR models typically uses <i>teacher forcing</i>, where ground-truth data are fed as
                            context to stabilize learning.
                            However, during inference the model is conditioned on its own predictions, creating a
                            training-inference mismatch
                            known as <i>exposure bias</i>. Errors can then accumulate across autoregressive steps,
                            degrading output quality.
                            Our preliminary study shows that teacher forcing greatly reduces the designability of
                            generated structures.
                            To mitigate this, we adapt <b>Noisy Context Learning (NCL)</b> (todo:cite?) and <b>Scheduled
                                Sampling (SS)</b> (todo:cite?), techniques from language and
                            image AR modeling, for PAR.
                        </p>
                        <p>
                            <strong>Noisy Context Learning (NCL)</strong> adds noise to the prior-scale inputs during
                            training to improve robustness,
                            while <strong>Scheduled Sampling (SS)</strong> gradually replaces ground-truth context with
                            model predictions to mitigate exposure bias.
                        </p>
                    </div>

                    <div class="container is-max-desktop content">
                        <figure class="table-container mt-5 mb-5">
                            <figcaption class="has-text-centered is-size-6 mb-3">
                                <b>Table 2:</b>
                                <strong>Mitigating exposure bias for PAR.</strong>
                                We adopted various training strategies to mitigate the exposure bias for multi-scale
                                autoregressive modeling.
                                These techniques are consistently effective in improving structure quality.
                                NCL: Noisy Context Learning. SS: Scheduled Sampling.
                            </figcaption>

                            <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
                                <thead>
                                    <tr>
                                        <th class="has-text-centered">Method</th>
                                        <th class="has-text-centered">sc-RMSD ‚Üì</th>
                                        <th class="has-text-centered">FPSD vs.<br>(PDB / AFDB) ‚Üì</th>
                                        <th class="has-text-centered">fS<br>(C / A / T ‚Üë)</th>
                                    </tr>
                                </thead>

                                <tbody>
                                    <tr>
                                        <td>Teacher Forcing</td>
                                        <td class="has-text-centered">2.20</td>
                                        <td class="has-text-centered">99.66 / 37.64</td>
                                        <td class="has-text-centered">2.53 / 5.56 / 29.67</td>
                                    </tr>

                                    <tr>
                                        <td>+ NCL</td>
                                        <td class="has-text-centered">1.58</td>
                                        <td class="has-text-centered">89.70 / 23.69</td>
                                        <td class="has-text-centered">2.54 / 5.85 / 28.37</td>
                                    </tr>

                                    <tr>
                                        <td>+ NCL & SS</td>
                                        <td class="has-text-centered">1.48</td>
                                        <td class="has-text-centered">90.66 / 24.59</td>
                                        <td class="has-text-centered">2.54 / 5.84 / 28.77</td>
                                    </tr>
                                </tbody>
                            </table>
                        </figure>
                    </div>

                    <h3 class="title is-4" id="inference"> Inference</h3>
                    <div class="content has-text-justified">
                        <p>
                            At inference, the autoregressive transformer first produces
                            z<sup>1</sup> at the coarsest scale, which conditions the flow matching decoder
                            to generate x<sup>1</sup> either via ODE or SDE sampling.
                            We upsample x<sup>1</sup> using Up(x<sup>1</sup>, size(2)) and send it back into the
                            autoregressive transformer to predict the next scale embedding
                            z<sup>2</sup>. This coarse-to-fine process iterates n times until the flow-matching
                            model generates the full-resolution backbone x.
                            KV cache is applied throughout the autoregressive process for efficiency.
                        </p>

                        <p>
                            Leveraging the learned flow network v<sub>Œ∏</sub>, sampling could be performed at each scale
                            through ordinary differential equation (ODE)
                            d x<sub>t</sub> = v<sub>Œ∏</sub>(x<sub>t</sub>, t) d t,
                            with the scale superscript i omitted for simplicity.
                            Moreover, we could define the stochastic differential equation (SDE) for sampling:
                        </p>

                        <div
                            style="border-left: 4px solid #ccc; padding: 1em; background-color: #f9f9f9; font-weight: bold;">
                            $$
                            d x_t
                            = v_{\theta}(x_t, t)\, dt
                            + g(t)\, s_{\theta}(x_t, t)\, dt
                            + \sqrt{2 g(t)\, \gamma}\, d \mathcal{W}_t
                            $$
                        </div>

                        <p>
                            where <i>g</i>(t) is a time-dependent scaling function for the score function
                            <i>s</i><sub>Œ∏</sub>(x<sub>t</sub>, t), and the noise term Œ≥ is a noise scaling parameter,
                            and ùí≤<sub>t</sub> is a standard Wiener process.
                            The score function, defined as the gradient of the log-probability of the noisy data
                            distribution at time <i>t</i>, could be computed as:
                        </p>

                        <div
                            style="border-left: 4px solid #ccc; padding: 1em; background-color: #f9f9f9; font-weight: bold;">
                            $$
                            s_{\theta}(x_t, t)
                            = \frac{t\, v_{\theta}(x_t, t) - x_t}{1 - t}
                            $$
                        </div>

                    </div>
                </div>

                <div>
                    <h2 class="title is-3" id="par"> Experiments</h2>

                    <h3 class="title is-4" id="uncond"> Protein Backbone Generation</h3>
                    
                    <div class="content has-text-justified">

                        <p>
                            We compare 3-scale PAR‚Äôs (<i>S = {64, 128, 256}</i>) backbone generation performance with
                            other
                            baselines in Tab. 3, following the evaluation protocol in prior work.
                            The baselines span three categories: frame-based diffusion methods, multimodal protein
                            language
                            models, and diffusion/flow-based CŒ± generators.
                            We train both PAR and Proteina for 200k steps on our training data, and report results of
                            other
                            baselines for fair comparison.
                            Evaluation metrics and baseline categories are detailed in ¬ßA.2‚ÄìA.3.
                            As shown in Tab. 3,

                            <br>
                            üìå <strong> Designability & FPSD:</strong> PAR generates samples that closely match the
                            reference data distribution and maintain competitive designability.
                            PAR achieves high FPSD and fold score (fS), which assess generation quality and diversity by
                            comparing generated and reference distributions.
                            When comparing to Proteina, which uses non-equivariant transformers, PAR more effectively
                            models
                            the data distribution, achieving a lower FPSD (231.5 vs. 271.3).
                            By reducing the noise scaling parameter Œ≥ from 0.45 to 0.30 in Equation 6 for the SDE
                            sampling,
                            we can reduce sampling stochasticity and improve sample quality, improving the designability
                            from 88.0% to 96.0% without additional training.

                            <br>

                            üìå <strong> Diversity:</strong> Moreover, PAR demonstrates better generation diversity among
                            all
                            methods.

                            <br>

                            üìå <strong> PDB Fine-tuning:</strong>
                            Meanwhile, prior work also reports results using fine-tuned models on a PDB subset, which
                            improves FPSD while maintaining strong designability.
                            Following their approach, we curated a PDB subset of 21k designable samples and fine-tuned
                            PAR
                            on this subset for 5k steps.
                            After PDB fine-tuning, PAR achieved improved performance across all metrics.
                            <br>
                        </p>
                    </div>

                    <div class="container is-max-desktop content">
                        <figure class="table-container mt-5 mb-5">
                            <figcaption class="has-text-centered is-size-6 mb-3">
                                <b>Table 3:</b>
                                <strong>Unconditional backbone generation performance.</strong>
                                We follow FPSD and fS to evaluate the model‚Äôs ability to capture the data distribution.
                                <span>PAR<sub>pdb</sub> denotes the 400M model finetuned on the PDB subset.</span>
                            </figcaption>

                            <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
                                <thead>
                                    <tr>
                                        <th class="has-text-centered">Method</th>
                                        <th class="has-text-centered">Designability<br>(% ‚Üë)</th>
                                        <th class="has-text-centered">sc-RMSD ‚Üì</th>
                                        <th class="has-text-centered">Diversity<br>TM-Sc. ‚Üì</th>
                                        <th class="has-text-centered">FPSD vs.<br>PDB ‚Üì</th>
                                        <th class="has-text-centered">FPSD vs.<br>AFDB ‚Üì</th>
                                        <th class="has-text-centered">fS<br>(C / A / T ‚Üë)</th>
                                        <th class="has-text-centered">Sec. Struct. %<br>(Œ± / Œ≤)</th>
                                    </tr>
                                </thead>

                                <tbody>

                                    <tr>
                                        <td>FrameDiff (17M)</td>
                                        <td class="has-text-centered">65.4</td>
                                        <td class="has-text-centered">-</td>
                                        <td class="has-text-centered">0.40</td>
                                        <td class="has-text-centered">194.2</td>
                                        <td class="has-text-centered">258.1</td>
                                        <td class="has-text-centered">2.46 / 5.78 / 23.35</td>
                                        <td class="has-text-centered">64.9 / 11.2</td>
                                    </tr>

                                    <tr>
                                        <td>RFDiffusion (60M)</td>
                                        <td class="has-text-centered">94.4</td>
                                        <td class="has-text-centered">-</td>
                                        <td class="has-text-centered">0.42</td>
                                        <td class="has-text-centered">253.7</td>
                                        <td class="has-text-centered">252.4</td>
                                        <td class="has-text-centered">2.25 / 5.06 / 21.48</td>
                                        <td class="has-text-centered">66.3 / 9.2</td>
                                    </tr>

                                    <tr>
                                        <td>ESM3 (1.4B)</td>
                                        <td class="has-text-centered">22.0</td>
                                        <td class="has-text-centered">-</td>
                                        <td class="has-text-centered">0.42</td>
                                        <td class="has-text-centered">933.9</td>
                                        <td class="has-text-centered">855.4</td>
                                        <td class="has-text-centered">3.19 / 6.71 / 17.73</td>
                                        <td class="has-text-centered">72.7 / 4.8</td>
                                    </tr>

                                    <tr>
                                        <td>Genie2 (16M)</td>
                                        <td class="has-text-centered">95.2</td>
                                        <td class="has-text-centered">-</td>
                                        <td class="has-text-centered">0.38</td>
                                        <td class="has-text-centered">350.0</td>
                                        <td class="has-text-centered">313.8</td>
                                        <td class="has-text-centered">1.55 / 3.66 / 11.65</td>
                                        <td class="has-text-centered">72.7 / 4.8</td>
                                    </tr>

                                    <tr>
                                        <td>Proteina (200M)</td>
                                        <td class="has-text-centered">92.8</td>
                                        <td class="has-text-centered">1.14</td>
                                        <td class="has-text-centered">0.37</td>
                                        <td class="has-text-centered">282.3</td>
                                        <td class="has-text-centered">285.6</td>
                                        <td class="has-text-centered">2.17 / 6.22 / 21.48</td>
                                        <td class="has-text-centered">66.3 / 9.2</td>
                                    </tr>

                                    <tr>
                                        <td>Proteina (400M)</td>
                                        <td class="has-text-centered">92.6</td>
                                        <td class="has-text-centered">1.09</td>
                                        <td class="has-text-centered">0.37</td>
                                        <td class="has-text-centered">271.3</td>
                                        <td class="has-text-centered">272.6</td>
                                        <td class="has-text-centered">2.13 / 6.14 / 21.48</td>
                                        <td class="has-text-centered">65.1 / 9.7</td>
                                    </tr>

                                    <tr>
                                        <td>Proteina<sub>pdb</sub></td>
                                        <td class="has-text-centered">94.8</td>
                                        <td class="has-text-centered">1.02</td>
                                        <td class="has-text-centered">0.36</td>
                                        <td class="has-text-centered">181.5</td>
                                        <td class="has-text-centered">257.3</td>
                                        <td class="has-text-centered">2.64 / 6.48 / 30.10</td>
                                        <td class="has-text-centered">46.9 / 17.6</td>
                                    </tr>

                                    <tr>
                                        <td>PAR (200M)</td>
                                        <td class="has-text-centered">87.0</td>
                                        <td class="has-text-centered">1.33</td>
                                        <td class="has-text-centered">0.37</td>
                                        <td class="has-text-centered">252.0</td>
                                        <td class="has-text-centered">237.9</td>
                                        <td class="has-text-centered">2.11 / 6.14 / 19.22</td>
                                        <td class="has-text-centered">64.3 / 8.8</td>
                                    </tr>

                                    <tr>
                                        <td>PAR (400M)</td>
                                        <td class="has-text-centered">88.0</td>
                                        <td class="has-text-centered">1.28</td>
                                        <td class="has-text-centered">0.36</td>
                                        <td class="has-text-centered">231.5</td>
                                        <td class="has-text-centered">211.8</td>
                                        <td class="has-text-centered">2.20 / 6.50 / 20.96</td>
                                        <td class="has-text-centered">63.8 / 10.4</td>
                                    </tr>

                                    <tr>
                                        <td>Œ≥ = 0.30</td>
                                        <td class="has-text-centered">96.0</td>
                                        <td class="has-text-centered">1.01</td>
                                        <td class="has-text-centered">0.39</td>
                                        <td class="has-text-centered">313.9</td>
                                        <td class="has-text-centered">296.4</td>
                                        <td class="has-text-centered">2.24 / 6.60 / 16.71</td>
                                        <td class="has-text-centered">52.3 / 13.1</td>
                                    </tr>

                                    <tr>
                                        <td>PAR<sub>pdb</sub></td>
                                        <td class="has-text-centered">96.6</td>
                                        <td class="has-text-centered">1.04</td>
                                        <td class="has-text-centered">0.43</td>
                                        <td class="has-text-centered">161.0</td>
                                        <td class="has-text-centered">228.4</td>
                                        <td class="has-text-centered">2.57 / 7.42 / 23.61</td>
                                        <td class="has-text-centered">50.2 / 16.7</td>
                                    </tr>

                                </tbody>
                            </table>
                        </figure>
                    </div>

                    <h3 class="title is-4" id="prompt"> 0-shot Prompted Generation</h3>
                    
                    <div class="content has-text-justified">
                        <p>
                            Proteins possess hierarchical and complex structures, which makes it challenging to directly specify 
                            a target shape and design proteins accordingly. By leveraging PAR‚Äôs coarse-to-fine generation, 
                            a simple prompt (e.g., 16 points) can specify a protein‚Äôs coarse layout, from which the model generates 
                            the complete structure as shown in Fig. 3.
                            </p>
                            
                            <p>
                            In particular, we first obtain a 16-point input prompt either by downsampling a real protein structure 
                            from the test set or by specifying the points manually (the top row in Fig. 3). Using a 5-scale PAR 
                            (<i>S = {16, 32, 64, 128, 256}</i>), we initialize the first-scale prediction with the 16-point prompt 
                            and autoregressively upsample until the full protein structure is generated, as illustrated in the bottom row of Fig. 3. 
                            <br>
                            üìå Following this process, PAR can generate a new structure that <strong>preserves the coarse structural layout</strong> 
                            (first five examples), and <strong>explore entirely novel structures</strong> (last three examples). 
                            <br>
                            If desired, longer prompts (e.g., 32 points) could be specified to achieve more finer-grained control 
                            over backbone generation.
                            <br>
                            </p>
                    </div>

                    <figure style="margin-top: 1rem; margin-bottom: 1rem;">
                        <img src="./static/img/prompt_shape-crop.png" style="width:100%;">

                        <!-- Add the figcaption below the image -->
                        <figcaption class="has-text-centered is-size-7 mt-2">
                            <p>
                                <strong>Figure 3: Backbone generation with human prompt.</strong>
                                Given a small number of points (e.g., 16) as prompt, PAR can generate protein backbones that adhere
                                to the global arrangements specified by these points, <em>without</em> any finetuning.
                                For visualization, input points are interpolated to match the length of the generated structure.
                            </p>
                                
                        </figcaption>
                    </figure>

                    <h3 class="title is-4" id="motif"> 0-shot Motif Scaffolding</h3>
                    <div class="content has-text-justified">
                        <p>
                            Besides the point-based layout, PAR can preserve finer-grained prompts like atomic coordinates. 
                            Fig. 4 highlights the zero-shot motif scaffolding capabilities of PAR. 
                            Using a 5-scale PAR, we downsample a raw protein structure into five scales and teacher-force the 
                            ground-truth motif coordinates at each scale before propagating into the next scale. 
                            To avoid clashes or discontinuities, we superimpose the ground-truth motif residues and the generated motif segments before replacement. 
                            <br>
                            üìå With no fine-tuning and no conditioning, PAR generates <strong>plausible scaffolds</strong> that preserve motif structures with high fidelity. 
                            <br>
                            This stands in contrast to diffusion or flow-based frameworks, which typically require fine-tuning on additional conditions such as masks or motif coordinates, or rely on decomposition strategies.
                            </p>
                            
                            <p>
                            Moreover, the generated scaffolds differ substantially from the input structure, showing that PAR generates structurally diverse scaffolds rather than merely copying. 
                            For example, the leftmost example in Fig. 4 preserves the yellow motif helix while introducing new secondary structure elements such as Œ≤-sheet and loops, in contrast to the original helices.
                            </p>
                            
                    </div>

                    <figure style="margin-top: 1rem; margin-bottom: 1rem;">
                        <img src="./static/img/motif_scaffold.png" style="width:100%;">

                        <!-- Add the figcaption below the image -->
                        <figcaption class="has-text-centered is-size-7 mt-2">
                            <p>
                                <strong>Figure 4: Zero-shot motif scaffolding.</strong>
                                Given a motif structure, PAR can generate diverse, plausible scaffold structures that accurately preserve 
                                the motif via teacher-forcing the motif coordinates at each scale, without additional conditioning or fine-tuning.
                            </p>
                                
                        </figcaption>
                    </figure>

                    <h3 class="title is-4" id="scaling"> Scaling Effect of PAR</h3>
                    <div class="content has-text-justified">
                        <p>
                            We examine the model‚Äôs behaviors by varying the backbone decoder‚Äôs size and number of training 
                            steps in Fig. 5. We train PAR with 3 scales over three different model sizes with 60M, 200M, 
                            and 400M parameters and three training durations over 200K, 400K, and 600K steps. 
                            <br>
                            üìå PAR demonstrates favorable behavior when scaling both model size and training duration,
                            effectively <strong>improving its ability to capture the protein data distribution</strong> with FPSD scores of 
                            187 against PDB and 170 against AFDB (first two columns in Fig. 5). 
                            Further, the fS scores, which reflect <strong>quality and diversity</strong>, increase with larger model sizes 
                            and greater computational budgets.
                            <br>
                            </p>
                            
                            <p>
                            While extending training duration alone offers negligible gains, increasing model size 
                            substantially enhances designability, leading to lower sc-RMSD values. Meanwhile, we empirically 
                            observe that scaling the autoregressive transformer has minimal impacts on the evaluation results. 
                            This is consistent with the module‚Äôs role to generate scale-wise conditioning to guide the backbone 
                            generation, which does not require large model capacity. Similar trends have been observed in image 
                            generation, where outputs from one scale are directly passed to the next without relying on additional 
                            encoding modules. This allows us to prioritize increasing the backbone decoder‚Äôs model capacity that 
                            effectively improves metrics.
                            </p>
                            
                    </div>

                    <figure style="margin-top: 1rem; margin-bottom: 1rem;">
                        <img src="./static/img/scaling.png" style="width:100%;">

                        <!-- Add the figcaption below the image -->
                        <figcaption class="has-text-centered is-size-7 mt-2">
                            <p>
                                <strong>Figure 5: Scaling effects of PAR.</strong>
                                Performance of four metrics over varying training steps and model sizes:
                                (a) FPSD vs. PDB, 
                                (b) FPSD vs. AFDB, 
                                (c) fS(T), 
                                (d) sc-RMSD.
                                </p>
                                
                        </figcaption>
                    </figure>

                    <h3 class="title is-4" id="efficiency"> Inference Efficiency</h3>
                    <div class="content has-text-justified">
                        <p>
                            While <strong>Tab. 3</strong> reports results using a uniform number of sampling steps across scales, 
                            the multi-scale formulation of PAR actually offers advantages in sampling efficiency. 
                            More specifically, (1) sampling at the coarser scale (e.g., first scale) is more efficient than 
                            sampling at finer scales (e.g., 2nd scale) due to shorter sequence length; 
                            (2) we can use fewer sampling steps at finer scales than coarser scales. 
                            
                            <br>

                            üìå <strong> 2√ó Inference Speedup:</strong> As shown in Tab. 4, by using SDE sampling only at the first scale,
                            and switching to ODE sampling for the remaining scales, PAR could dramatically reduce 
                            the diffusion steps from 400 to 2 steps at the last two scales without harming designability (97%), 
                            yielding a 2√ó inference speedup. 
                            
                            <br>

                            This is possible because a high-quality coarse topology places 
                            the model near high-density regions, enabling efficient refinement with ODE sampling.
                        </p>
                        
                        <p>
                            <!-- Naively reducing the SDE sampling steps significantly harms designability, dropping to 22%  -->
                            <!-- when reducing steps to 50, as shown in Fig. 8.  -->
                            This is consistent with the observation of single-scale models like Proteina, 
                            where designability degrades to 89% when reducing SDE sampling steps to 200 in Tab. 4. 
                            Crucially, SDE sampling at the first scale is necessary for establishing a reliable global topology, 
                            given that ODE-only sampling exhibits poor designability. 
                        </p>
                        
                        <p>
                            Compared to the single-scale 400-step baseline, PAR achieves 1.96√ó and 2.5√ó sampling speedup 
                            at length 150 and 200, respectively. 
                            This improvement is driven by speeding up the final scales, where the longer sequence lengths 
                            cause computational costs to grow quadratically in transformer architectures. 
                            Moreover, the computational costs remain constant at the first scale because it has a fixed size 64, 
                            even when generating longer sequences.
                        </p>
                        
                    </div>

                    <div class="container is-max-desktop content">
                        <figure class="table-container mt-5 mb-5">
                          <figcaption class="has-text-centered is-size-6 mb-3">
                            <b>Table 4:</b>
                            <strong>Performance of different sampling methods and steps.</strong>
                            Combining SDE and ODE sampling across scales yields a 
                            \(2.5\times\) inference speedup compared to the single-scale 
                            400-step baseline, shown in the first and the last row.  
                            We generate 100 samples at each length.
                          </figcaption>
                      
                          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
                            <thead>
                              <tr>
                                <th rowspan="2" class="is-vcentered has-text-centered">Sampling</th>
                                <th rowspan="2" class="is-vcentered has-text-centered">Steps</th>
                                <th colspan="2" class="has-text-centered">Length 150</th>
                                <th colspan="2" class="has-text-centered">Length 200</th>
                              </tr>
                              <tr>
                                <th class="has-text-centered">Time (s)</th>
                                <th class="has-text-centered">Design. (%)</th>
                                <th class="has-text-centered">Time (s)</th>
                                <th class="has-text-centered">Design. (%)</th>
                              </tr>
                            </thead>
                      
                            <tbody>
                              <!-- Proteina SDE -->
                              <tr>
                                <td rowspan="2">Proteina (SDE)</td>
                                <td>0/0/400</td>
                                <td class="has-text-centered">131</td>
                                <td class="has-text-centered">97%</td>
                                <td class="has-text-centered">170</td>
                                <td class="has-text-centered">92%</td>
                              </tr>
                              <tr>
                                <td>0/0/200</td>
                                <td class="has-text-centered">67</td>
                                <td class="has-text-centered">89%</td>
                                <td class="has-text-centered">86</td>
                                <td class="has-text-centered">80%</td>
                              </tr>
                      
                              <!-- All SDE -->
                              <tr>
                                <td rowspan="2">All SDE</td>
                                <td>400/400/400</td>
                                <td class="has-text-centered">312</td>
                                <td class="has-text-centered">97%</td>
                                <td class="has-text-centered">351</td>
                                <td class="has-text-centered">94%</td>
                              </tr>
                              <tr>
                                <td>400/400/2</td>
                                <td class="has-text-centered">184</td>
                                <td class="has-text-centered">0%</td>
                                <td class="has-text-centered">‚Äì</td>
                                <td class="has-text-centered">‚Äì</td>
                              </tr>
                      
                              <!-- All ODE -->
                              <tr>
                                <td>All ODE</td>
                                <td>400/400/400</td>
                                <td class="has-text-centered">312</td>
                                <td class="has-text-centered">28%</td>
                                <td class="has-text-centered">‚Äì</td>
                                <td class="has-text-centered">‚Äì</td>
                              </tr>
                      
                              <!-- S/S/O -->
                              <tr>
                                <td rowspan="2">S/S/O</td>
                                <td>400/400/400</td>
                                <td class="has-text-centered">312</td>
                                <td class="has-text-centered">98%</td>
                                <td class="has-text-centered">‚Äì</td>
                                <td class="has-text-centered">‚Äì</td>
                              </tr>
                              <tr>
                                <td>400/400/2</td>
                                <td class="has-text-centered">184</td>
                                <td class="has-text-centered">99%</td>
                                <td class="has-text-centered">186</td>
                                <td class="has-text-centered">91%</td>
                              </tr>
                      
                              <!-- S/O/O -->
                              <tr>
                                <td rowspan="2">S/O/O</td>
                                <td>400/400/400</td>
                                <td class="has-text-centered">312</td>
                                <td class="has-text-centered">96%</td>
                                <td class="has-text-centered">‚Äì</td>
                                <td class="has-text-centered">‚Äì</td>
                              </tr>
                              <tr>
                                <td>400/2/2</td>
                                <td class="has-text-centered"><b>67</b></td>
                                <td class="has-text-centered">97%</td>
                                <td class="has-text-centered"><b>68</b></td>
                                <td class="has-text-centered">94%</td>
                              </tr>
                      
                            </tbody>
                          </table>
                        </figure>
                      </div>
                      

                    <h3 class="title is-4" id="attn"> Interpreting AR Transformer Attention</h3>
                    <div class="content has-text-justified">

                        <p>
                            We visualize the attention maps of the autoregressive transformer at each scale (Fig. 6). 
                            We average the attention scores within each scale, normalize them such that the scores across scales sum to 1, 
                            and average them over 50 test samples to obtain the scale-level attention distribution during inference.
                            We summarize three key observations:
                            </p>
                            
                            <p>
                            <i>(i)</i> Most scales barely attend to the first scale, since the input to this scale, a <code>bos</code> token, carries little structural signal.  
                            </p>
                            
                            <p>
                            <i>(ii)</i> Each scale primarily attends to the previous scale, which typically contains richer contextual and structural information.
                            </p>
                            
                            <p>
                            <i>(iii)</i> Despite focusing most heavily on the current scale, the model still retains non-negligible attention to earlier scales.
                            <br>
                            üìå This indicates that PAR effectively integrates information across multiple scales and <strong>maintains structural consistency</strong> during generation.
                            <br>
                            </p>
                            
                    </div>

                    <figure style="margin-top: 1rem; margin-bottom: 1rem;">
                        <img src="./static/img/attn_map.png" style="width:80%;">

                        <!-- Add the figcaption below the image -->
                        <figcaption class="has-text-centered is-size-7 mt-2">
                            <p>
                                <strong>Figure 6:</strong> Visualization of the average attention scores in PAR autoregressive transformer over 5 scales,
                                obtained from samples with lengths in (128, 256). 
                                We provide attention map visualization for shorter proteins in ¬ßC.3.
                            </p>                            
                                
                        </figcaption>
                    </figure>

                </div>

                <h2 class="title is-3" id="conclusion"> Conclusion </h2>
                <div class="content has-text-justified">
                    <p>
                        PAR is the first multi-scale autoregressive model for protein backbone generation, offering a general framework that includes flow-based methods as a special case. PAR addressed limitations of standard autoregressive models, such as unidirectional dependency, discretization, and exposure bias. Our method robustly models structures over multiple granularities and in turn enables strong zero-shot generalization. This capability includes coarse-prompted conditional generation using points (e.g., 16 points) as structural layout and finer-grained controls such as atomic-coordinate-based motif scaffolding. For unconditional backbone generation, PAR exhibits powerful distributional fidelity and generation quality. The analysis of scale-level attention map provides additional insights into how the multi-scale formulation operates.
                        </p>
                        
                        <p>
                        We hope that PAR unlocks the potential of autoregressive modeling for protein design. Some promising open directions include:
                        </p>
                        
                        <p>
                        (1) <strong>Conformational dynamics modeling.</strong> PAR can, in principle, perform zero-shot modeling of conformational distributions: we downsample a structure and upsample it with PAR to mimic local molecular dynamics. We leave this exciting application for future research.
                        </p>
                        
                        <p>
                        (2) <strong>All-atom modeling.</strong> This work focuses on backbone CŒ± atoms to prioritize autoregressive design, but it is natural to extend to full-atom representations. The multi-scale framework offers an advantage for flexible zero-shot prompt-based all-atom designs.
                        </p>
                        
                </div>
            </div>
        </div>


    </section>

    <style>
        .video-grid-two-cols {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            /* Two columns */
            gap: 10px;
            /* Gap between videos */
            width: 60%;
            /* Set the container width to 60% */
            margin: 0 auto;
            /* Center the container horizontally */
        }

        .video-grid-two-cols video {
            width: 100%;
            /* Videos fill the container width */
            height: auto;
        }
    </style>




    <!-- <section class="section" id="Acknowledgements">
      <div class="container is-max-desktop content">
        <h2 class="title is-3">Acknowledgements</h2>
        The data collection efforts behind <a href="https://taodataset.org/">TAO</a> dataset are crucial for the
        realization of TAO-Amodal.
        We also thank <a href="https://github.com/Ali2500/BURST-benchmark">BURST</a> dataset for its collection of modal
        mask annotations.
        Amodal annotations for this dataset were provided by AnnotateX. We thank Neehar Peri and Jason Zhang from CMU
        for their detailed feedback on the dataset and experiments.
      </div>
    </section> -->


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title is-3" id="citation">BibTeX</h2>
            <pre><code>
      @article{par,
          title={Protein Autoregressive Modeling via Multiscale Structure Generation},
      }
          </code></pre>
        </div>
    </section>



    <footer class="footer">
        <div align="center" class="container">
            <div class="columns is-centered">
                <div class="content">
                    This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                    You are free to borrow the source code of this website, we just ask that you link back to this page
                    in the footer.
                    <br> This website is licensed under a <a rel="license"
                        href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                </div>
            </div>
        </div>
    </footer>


</body>

</html>